{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# AutoComplete\n",
    "### Aim: \n",
    "To suggest top k words given a substring\n",
    "### Solution: \n",
    "- Split each word into n-grams\n",
    "- Sort words based on distance between given word and each candidate. Implemented distances are\n",
    "    - Jaccard distance : Intersection of ngrams over Union of ngrams\n",
    "    - Intersection of ngrams : Number of ngrams common between word and candidate\n",
    "- Return top k words\n",
    "- Additionally, given a corpus such as a user's chat history, the k words could further be sorted based on frequency in corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Importing words from corpus**\n",
    "<br>Omitting words shorter than 4 letters, so that a minimum of 2 trigrams are extracted for every word."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of words in corpus : 229483\n",
      "Top 10 words (alphabetically):\n",
      "[u'aalii', u'aardvark', u'aardwolf', u'aaron', u'aaronic', u'aaronical', u'aaronite', u'aaronitic', u'ababdeh', u'ababua']\n"
     ]
    }
   ],
   "source": [
    "from nltk import jaccard_distance,ngrams\n",
    "from nltk.corpus import words\n",
    "word_list = words.words()\n",
    "reduced_wlist = [w.lower() for w in word_list if len(w)>4] \n",
    "print \"Number of words in corpus :\",len(reduced_wlist)\n",
    "print \"Top 10 words (alphabetically):\\n\",reduced_wlist[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Reducing sample space for each word by filtering words that start with same substring as the word**\n",
    "<br>`compare_len` : Number of characters of each word that will be matched\n",
    "\n",
    "\n",
    "\n",
    "Note : Though this step reduces the sample space, it also assumes that the first few characters of the word have been spelled right. \n",
    "Hence it will not be able to handle typos. \n",
    "For example, if the user types *cake* instead of *bake*, the function cannot suggest *bakery*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_candidates(wlist,word,compare_len=1):\n",
    "    return [w for w in wlist if w[:compare_len] == word[:compare_len]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Get distance between word and candidate. **\n",
    "Implemented distances are\n",
    "- Jaccard distance : Intersection of ngrams over Union of ngrams\n",
    "- Intersection of ngrams : Number of ngrams common between word and candidate\n",
    "        \n",
    "<br>Note : Since Jaccard Distance will penalise longer candidates and favour shorter candidates with more ngrams common and fewer uncommon ngrams."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def distance(candidate,word,jacc = True):\n",
    "    if jacc:\n",
    "        return jaccard_distance(set(ngrams(word, n=4)), set(ngrams(candidate, n=4)))\n",
    "    else:\n",
    "        return len(set(ngrams(word, n=4)).intersection(set(ngrams(candidate, n=4))))*-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Suggest words**\n",
    "<br>Get candidates and sort them by distance. Return top k candidates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(word,k=5):\n",
    "    word = word.lower()\n",
    "    candidates = get_candidates(reduced_wlist,word)\n",
    "    preds = sorted(candidates, key=lambda x:distance(x,word,False))[:k]\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exac : [u'exacerbate', u'exacerbation', u'exacerbescence', u'exacerbescent', u'exact']\n",
      "Amaz : [u'amaze', u'amazed', u'amazedly', u'amazedness', u'amazeful']\n",
      "Bril : [u'brill', u'brilliance', u'brilliancy', u'brilliandeer', u'brilliant']\n"
     ]
    }
   ],
   "source": [
    "for w in ['Exac','Amaz','Bril']:\n",
    "    print w,\":\",predict(w,5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todo\n",
    "- Use a corpus such as chat history to get frequency of each word and use that as well to sort candidates\n",
    "- Train a sequential model on a corpus (after embedding words) to also rank suggestions based on the words that occured before it (CBOW)\n",
    "- Since one of the most common uses of AutoComplete is to fill longer words rather than shorter ones, consider ranking words by descending order of word length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
